{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0cde43-d4fb-4fbc-ac7c-f181ead823c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import amp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
    "import os\n",
    "from collections import Counter\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5e177-01b2-4690-82a8-eefca78e6c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoTransform:\n",
    "    def __init__(self, image_size=224, is_train=True):\n",
    "        self.image_size = image_size\n",
    "        self.is_train = is_train\n",
    "        self.mean = [0.5, 0.5, 0.5]\n",
    "        self.std = [0.5, 0.5, 0.5]\n",
    "    \n",
    "    def __call__(self, frames):\n",
    "        # frames đầu vào: [T, C, H, W]\n",
    "        if self.is_train:\n",
    "            h, w = frames.shape[-2:]\n",
    "            scale = random.uniform(0.8, 1.0)\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            \n",
    "            frames = TF.resize(frames, [new_h, new_w], interpolation=InterpolationMode.BILINEAR)\n",
    "            \n",
    "            i = random.randint(0, max(0, new_h - self.image_size))\n",
    "            j = random.randint(0, max(0, new_w - self.image_size))\n",
    "            \n",
    "            frames = TF.crop(frames, i, j, self.image_size, self.image_size)\n",
    "            if random.random() < 0.5:\n",
    "                frames = TF.hflip(frames)\n",
    "            \n",
    "            if random.random() < 0.3:\n",
    "                frames = TF.adjust_brightness(frames, random.uniform(0.9, 1.1))\n",
    "            if random.random() < 0.3:\n",
    "                frames = TF.adjust_contrast(frames, random.uniform(0.9, 1.1))\n",
    "        else:\n",
    "            frames = TF.resize(frames, [self.image_size, self.image_size], interpolation=InterpolationMode.BILINEAR)\n",
    "        \n",
    "        # Chuẩn hóa từng frame trong 1 tensor 4D [T, C, H, W]\n",
    "        # Chú ý: TF.normalize có thể nhận tensor 4D nếu version torchvision đủ mới, \n",
    "        # nhưng an toàn nhất là làm thế này:\n",
    "        frames = frames / 255.0 if frames.max() > 1.0 else frames\n",
    "        \n",
    "        # Biến đổi mean/std thành dạng [1, 3, 1, 1] để broadcast trừ cho [T, 3, H, W]\n",
    "        m = torch.tensor(self.mean).view(1, 3, 1, 1)\n",
    "        s = torch.tensor(self.std).view(1, 3, 1, 1)\n",
    "        frames = (frames - m) / s\n",
    "        \n",
    "        return frames\n",
    "\n",
    "print(\"Augmentation defined\")\n",
    "\n",
    "def denormalize(video_tensor, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n",
    "    \"\"\"\n",
    "    Biến đổi tensor từ chuẩn hóa [-1, 1] hoặc [custom] về lại [0, 1] để hiển thị.\n",
    "    Dữ liệu đầu vào: video_tensor dạng [C, T, H, W]\n",
    "    \"\"\"\n",
    "    # 1. Tạo tensor mean và std với định dạng [C, 1, 1, 1] để khớp với [C, T, H, W]\n",
    "    device = video_tensor.device\n",
    "    mean = torch.tensor(mean, device=device).view(3, 1, 1, 1)\n",
    "    std = torch.tensor(std, device=device).view(3, 1, 1, 1)\n",
    "    \n",
    "    # 2. Phép tính ngược: (x * std) + mean\n",
    "    res = video_tensor * std + mean\n",
    "    \n",
    "    # 3. Đảm bảo giá trị nằm trong khoảng [0, 1] để tránh lỗi hiển thị\n",
    "    res = torch.clamp(res, 0, 1)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ac2d3-48a5-42c2-ab6d-88a0bcd80733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root,num_segments=4,clip_len=16, image_size=224, is_train=True):\n",
    "        self.root = Path(root)\n",
    "        self.num_segments = num_segments\n",
    "        self.clip_len = clip_len\n",
    "        self.is_train = is_train\n",
    "        self.transform = VideoTransform(image_size, is_train)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {name: idx for idx, name in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        self.label_idx = []\n",
    "        for cls in self.classes:\n",
    "            cls_dir = self.root / cls\n",
    "            for video_dir in sorted([d for d in cls_dir.iterdir() if d.is_dir()]):\n",
    "                frame_paths = sorted([p for p in video_dir.iterdir() if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}])\n",
    "                if frame_paths:\n",
    "                    self.samples.append((frame_paths, self.class_to_idx[cls]))\n",
    "                    self.label_idx.append(self.class_to_idx[cls])\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _tsn_sample(self, frames):\n",
    "        total = len(frames)\n",
    "        seg_size = total / self.num_segments\n",
    "        clips = []\n",
    "\n",
    "        for i in range(self.num_segments):\n",
    "            start = int(i * seg_size)\n",
    "            end = int((i + 1) * seg_size)\n",
    "            if self.is_train:\n",
    "                center = np.random.randint(start, max(start + 1, end))\n",
    "            else:\n",
    "                center = (start + end) // 2\n",
    "\n",
    "            idxs = np.linspace(\n",
    "                max(0, center - self.clip_len // 2),\n",
    "                min(total - 1, center + self.clip_len // 2),\n",
    "                self.clip_len\n",
    "            ).astype(int)\n",
    "\n",
    "            clips.append([frames[i] for i in idxs])\n",
    "\n",
    "        return clips\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths, label = self.samples[idx]\n",
    "\n",
    "        clips = self._tsn_sample(frame_paths)\n",
    "\n",
    "        processed_clips = []\n",
    "        for clip in clips:\n",
    "            # Đọc tất cả ảnh vào 1 mảng numpy duy nhất [T, H, W, C]\n",
    "            video_array = np.stack([cv2.cvtColor(cv2.imread(str(p)), cv2.COLOR_BGR2RGB) for p in clip])\n",
    "            \n",
    "            # Chuyển sang Tensor [T, H, W, C]\n",
    "            video_tensor = torch.from_numpy(video_array).float()\n",
    "            video_tensor = video_tensor.permute(0, 3, 1, 2)\n",
    "            # Thực hiện Transform trên khối T-C-H-W\n",
    "            if self.transform:\n",
    "                video_tensor = self.transform(video_tensor)\n",
    "            \n",
    "            # Cuối cùng mới Permute sang chuẩn mô hình 3D: [C, T, H, W]\n",
    "            # Giả sử video_tensor lúc này là [T, H, W, C]\n",
    "            video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "            processed_clips.append(video_tensor)\n",
    "\n",
    "        video = torch.stack(processed_clips, dim=0)\n",
    "        return video, label\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root,num_segments=4,clip_len=16, image_size=224):\n",
    "        self.root = Path(root)\n",
    "        self.num_segments = num_segments\n",
    "        self.clip_len = clip_len\n",
    "        self.transform = VideoTransform(image_size, is_train=False)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.video_dirs = sorted([d for d in self.root.iterdir() if d.is_dir()], key=lambda x: int(x.name))\n",
    "        self.video_ids = [int(d.name) for d in self.video_dirs]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_dirs)\n",
    "    \n",
    "    def _tsn_sample(self, frames):\n",
    "        total = len(frames)\n",
    "        seg_size = total / self.num_segments\n",
    "        clips = []\n",
    "\n",
    "        for i in range(self.num_segments):\n",
    "            start = int(i * seg_size)\n",
    "            end = int((i + 1) * seg_size)\n",
    "            center = (start + end) // 2\n",
    "\n",
    "            idxs = np.linspace(\n",
    "                max(0, center - self.clip_len // 2),\n",
    "                min(total - 1, center + self.clip_len // 2),\n",
    "                self.clip_len\n",
    "            ).astype(int)\n",
    "\n",
    "            clips.append([frames[i] for i in idxs])\n",
    "\n",
    "        return clips\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir = self.video_dirs[idx]\n",
    "        video_id = self.video_ids[idx]\n",
    "        frame_paths = sorted([p for p in video_dir.iterdir() if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}])\n",
    "        total = len(frame_paths)\n",
    "\n",
    "        clips = self._tsn_sample(frame_paths)\n",
    "        processed_clips = []\n",
    "        for clip in clips:\n",
    "            # Đọc tất cả ảnh vào 1 mảng numpy duy nhất [T, H, W, C]\n",
    "            video_array = np.stack([cv2.cvtColor(cv2.imread(str(p)), cv2.COLOR_BGR2RGB) for p in clip])\n",
    "            \n",
    "            # Chuyển sang Tensor [T, H, W, C]\n",
    "            video_tensor = torch.from_numpy(video_array).float()\n",
    "            video_tensor = video_tensor.permute(0, 3, 1, 2)\n",
    "            # Thực hiện Transform trên khối T-C-H-W\n",
    "            if self.transform:\n",
    "                video_tensor = self.transform(video_tensor)\n",
    "            \n",
    "            # Cuối cùng mới Permute sang chuẩn mô hình 3D: [C, T, H, W]\n",
    "            # Giả sử video_tensor lúc này là [T, H, W, C]\n",
    "            video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "            processed_clips.append(video_tensor)\n",
    "\n",
    "        video = torch.stack(processed_clips, dim=0)\n",
    "        return video, video_id\n",
    "    \n",
    "\n",
    "print(\"Dataset classes defined\")\n",
    "\n",
    "def collate_fn(batch: List[Tuple[torch.Tensor, int]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    videos = torch.stack([item[0] for item in batch])\n",
    "    labels = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
    "    return videos, labels\n",
    "\n",
    "def create_balanced_sampler(dataset):\n",
    "    \"\"\"Create balanced sampler for imbalanced dataset\"\"\"\n",
    "    if hasattr(dataset, 'dataset'):\n",
    "        all_labels = [dataset.dataset.label_idx[i] for i in dataset.indices]\n",
    "    else:\n",
    "        all_labels = dataset.label_idx\n",
    "\n",
    "    class_counts = np.bincount(all_labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = [class_weights[label] for label in all_labels]\n",
    "    sample_weights = torch.FloatTensor(sample_weights)\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    print(f\"Balanced Sampler: class counts min={class_counts.min()}, max={class_counts.max()}\")\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e975e1-f5d8-483b-8e45-05b5a415fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X3DFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.hub.load(\n",
    "            \"facebookresearch/pytorchvideo\",\n",
    "            \"x3d_s\",\n",
    "            pretrained=True\n",
    "        )\n",
    "\n",
    "        # bỏ head hoàn toàn\n",
    "        self.head = self.model.blocks[-1]\n",
    "        self.blocks = self.model.blocks[:-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T, H, W)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # x lúc này là feature map 5D\n",
    "        # (B, C, T', H', W')\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatioTemporalTokenizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, B, N):\n",
    "        \"\"\"\n",
    "        x: (B*N, C, H, W)   ← output thực tế của X3D\n",
    "        \"\"\"\n",
    "        BN, C, H, W = x.shape\n",
    "        assert BN == B * N, \"Batch mismatch\"\n",
    "\n",
    "        x = x.view(B, N, C, H, W)\n",
    "        x = x.permute(0, 1, 3, 4, 2)   # (B, N, H, W, C)\n",
    "        x = x.reshape(B, N * H * W, C)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SpatioTemporalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_tokens=256):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, max_tokens, dim)\n",
    "        )\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embed[:, :x.size(1)]\n",
    "\n",
    "class SpatioTemporalTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=192,\n",
    "        depth=2,\n",
    "        heads=4,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_embed = SpatioTemporalPositionalEncoding(dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, tokens, D)\n",
    "        \"\"\"\n",
    "        x = self.pos_embed(x)\n",
    "        x = self.encoder(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "class X3D_SpatioTemporalTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=51):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = X3DFeatureExtractor()\n",
    "        self.tokenizer = SpatioTemporalTokenizer()\n",
    "\n",
    "        self.embed_dim = 192\n",
    "\n",
    "        self.st_transformer = SpatioTemporalTransformer(\n",
    "            dim=self.embed_dim,\n",
    "            depth=2,\n",
    "            heads=4\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, N, C, T, H, W)\n",
    "        \"\"\"\n",
    "        B, N, C, T, H, W = x.shape\n",
    "        x = x.view(B * N, C, T, H, W)\n",
    "\n",
    "        feat = self.backbone(x)  # (B*N, C, t', h', w')\n",
    "        feat = feat.mean(dim=2)\n",
    "        tokens = self.tokenizer(feat, B, N)  # (B, tokens, C)\n",
    "\n",
    "        tokens = self.st_transformer(tokens)\n",
    "\n",
    "        feat = tokens.mean(dim=1)  # global pooling\n",
    "\n",
    "        \n",
    "        return self.fc(feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f4e355-2989-4382-8d07-28a425976cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, device, grad_accum_steps=1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    optimizer.zero_grad()\n",
    "    progress = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    for batch_idx, (videos, labels) in enumerate(progress):\n",
    "        videos = videos.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "        #x, y_a, y_b, lam = mixup_video(videos, labels)\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=(device.type == 'cuda')):\n",
    "            #logits = model(x)\n",
    "            logits = model(videos)\n",
    "            #loss = lam * criterion(logits, y_a) + (1 - lam) * criterion(logits, y_b)\n",
    "            loss = criterion(logits, labels)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        loss_value = loss.item()\n",
    "        loss = loss / grad_accum_steps\n",
    "        scaler.scale(loss).backward()\n",
    "        should_step = ((batch_idx + 1) % grad_accum_steps == 0) or (batch_idx + 1 == len(loader))\n",
    "        if should_step:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        batch_size = videos.size(0)\n",
    "        total_loss += loss_value * batch_size\n",
    "        progress.set_postfix(loss=f\"{loss_value:.4f}\", acc=f\"{correct / max(total, 1):.4f}\")\n",
    "    avg_loss = total_loss / max(total, 1)\n",
    "    avg_acc = correct / max(total, 1)\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a25be-e022-49cd-9438-549e7638d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA_TRAIN = r'/kaggle/input/action-video/data/data_train'\n",
    "PATH_DATA_TEST = r'/kaggle/input/action-video/data/test'\n",
    "NUM_FRAMES = 16\n",
    "FRAME_STRIDE = 2\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 4 \n",
    "NUM_SEGMENTS = 16\n",
    "train_dataset = VideoDataset(PATH_DATA_TRAIN,num_segments=4,clip_len=16,is_train=True)\n",
    "balanced_sampler = create_balanced_sampler(train_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,sampler=balanced_sampler, num_workers=2, pin_memory=True)\n",
    "test_dataset = TestDataset(PATH_DATA_TEST,num_segments=4,clip_len=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'Train clips: {len(train_dataset)} | Test clips: {len(test_dataset)}')\n",
    "print(f'Train Class count: {len(train_dataset.classes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a62cc9-0805-47e6-810a-c137435fc281",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_classes(parent_dir):\n",
    "    classes = {}\n",
    "    for folder in os.listdir(parent_dir):\n",
    "        subdir = os.path.join(parent_dir, folder)\n",
    "        num_subdirs = sum(\n",
    "            1 for d in os.listdir(subdir)\n",
    "            if os.path.isdir(os.path.join(subdir, d))\n",
    "        )\n",
    "        classes[folder] = num_subdirs\n",
    "    return classes\n",
    "per_class_counts = count_classes(PATH_DATA_TRAIN)\n",
    "print(\"Samples per class:\", per_class_counts)\n",
    "\n",
    "# 2️⃣ Tạo weight\n",
    "num_classes = len(per_class_counts)\n",
    "total_samples = sum(per_class_counts.values())\n",
    "\n",
    "weights = [total_samples / (num_classes * per_class_counts[c]) for c in per_class_counts]\n",
    "\n",
    "# 3️⃣ Chuyển sang tensor\n",
    "class_weights = torch.tensor(weights, dtype=torch.float).to(DEVICE)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7009d7-7fe0-4de8-a4fd-e8074000a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "sample_frames, sample_label = train_dataset[sample_idx]\n",
    "\n",
    "# 1. Vì sample_frames là [N, C, T, H, W], ta chọn clip đầu tiên để hiển thị\n",
    "# Shape sau khi chọn: [C, T, H, W]\n",
    "first_clip = sample_frames[0] \n",
    "\n",
    "# 2. Denormalize clip này\n",
    "vis_frames = denormalize(first_clip).cpu()\n",
    "\n",
    "class_name = train_dataset.classes[sample_label]\n",
    "\n",
    "# 3. Lấy số lượng khung hình từ chiều T (chiều thứ 2)\n",
    "# vis_frames shape hiện tại là [C, T, H, W]\n",
    "num_frames_in_clip = vis_frames.shape[1] \n",
    "frames_to_show = min(num_frames_in_clip, 12)\n",
    "\n",
    "cols = 4\n",
    "rows = math.ceil(frames_to_show / cols)\n",
    "\n",
    "plt.figure(figsize=(12, 3 * rows))\n",
    "\n",
    "for i in range(frames_to_show):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    \n",
    "    # Lấy khung hình thứ i dọc theo chiều T: vis_frames[:, i, :, :]\n",
    "    # Kết quả trả về tensor 3 chiều [C, H, W] -> Sẵn sàng để permute\n",
    "    frame = vis_frames[:, i, :, :].permute(1, 2, 0).numpy()\n",
    "    \n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Frame {i + 1}')\n",
    "\n",
    "plt.suptitle(f'Sample clip (Segment 1) from class: {class_name}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1729067-fcf4-4749-83a4-df659379d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "#checkpoint_path = Path('/kaggle/working/x3d_s_vit_best.pt')\n",
    "#print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "#checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "#classes = checkpoint['classes']\n",
    "\n",
    "model = X3D_SpatioTemporalTransformer(num_classes=len(train_dataset.classes)).to(DEVICE)\n",
    "#model.load_state_dict(checkpoint['model'])\n",
    "backbone_params = model.backbone.parameters()\n",
    "transformer_params = model.st_transformer.parameters()\n",
    "head_params = model.fc.parameters()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights,label_smoothing=0.1)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": backbone_params, \"lr\": 1e-4},\n",
    "        {\"params\": transformer_params, \"lr\": 5e-4},\n",
    "        {\"params\": head_params, \"lr\": 1e-3},\n",
    "    ],\n",
    "    weight_decay=0.05,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=4, gamma=0.5\n",
    ")\n",
    "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "#print(f\"Optimizer: AdamW | Base LR: {BASE_LR} | Head LR: {HEAD_LR}\")\n",
    "best_acc = 0.0\n",
    "best_ckpt = Path('./x3d_s_vit_best.pt')\n",
    "last_ckpt = Path('./x3d_s_vit_last.pt')\n",
    "history = {'train_loss': [], 'train_acc': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scaler,\n",
    "        DEVICE,\n",
    "        grad_accum_steps=GRAD_ACCUM_STEPS,\n",
    "    )\n",
    "    scheduler.step()\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    torch.save({'model': model.state_dict(), 'acc': train_acc}, last_ckpt)\n",
    "    if train_acc > best_acc:\n",
    "        best_acc = train_acc\n",
    "        torch.save({'model': model.state_dict(), 'acc': best_acc}, best_ckpt)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}/{EPOCHS} | train_loss={train_loss:.4f} | train_acc={train_acc:.4f}'\n",
    "    )\n",
    "\n",
    "trained_model = model\n",
    "training_history = history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768ee50-8b2b-4ddd-a5e4-7997f71e3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRunning inference...\")\n",
    "predictions = []\n",
    "classes = train_dataset.classes\n",
    "with torch.no_grad():\n",
    "    for videos, video_ids in tqdm(test_loader, desc=\"Inference\"):\n",
    "        videos = videos.to(DEVICE)\n",
    "        logits = model(videos)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        for video_id, pred_idx in zip(video_ids.cpu().numpy(), preds.cpu().numpy()):\n",
    "            pred_class = classes[pred_idx]\n",
    "            predictions.append((video_id, pred_class))\n",
    "\n",
    "predictions.sort(key=lambda x: x[0])\n",
    "print(f\"\\nTotal predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120c4a8-967d-46bd-b244-aeed1dc64aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = Path('./submission.csv')\n",
    "with open(submission_path, 'w') as f:\n",
    "    f.write('id,class\\n')\n",
    "    for video_id, pred_class in predictions:\n",
    "        f.write(f'{video_id},{pred_class}\\n')\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
