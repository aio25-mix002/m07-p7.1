{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79caabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import amp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
    "from transformers import VideoMAEForVideoClassification\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c452690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAugmentation:\n",
    "    \"\"\"\n",
    "    Augmentation cho video - CONSISTENT across all frames\n",
    "    Chỉ dùng cho training, không dùng cho val/test\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=224,\n",
    "        crop_scale=(0.8, 1.0),\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        speed_range=(0.9, 1.1),\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225),\n",
    "        is_train=True,\n",
    "        erase_prob=0.25,          \n",
    "        erase_scale=(0.02, 0.2), \n",
    "        erase_ratio=(0.3, 3.3)    \n",
    "    ):\n",
    "        self.image_size = image_size\n",
    "        self.crop_scale = crop_scale\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        self.saturation = saturation\n",
    "        self.speed_range = speed_range\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.is_train = is_train\n",
    "        self.erase_prob = erase_prob\n",
    "        self.erase_scale = erase_scale\n",
    "        self.erase_ratio = erase_ratio\n",
    "\n",
    "    def __call__(self, frames):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frames: tensor (T, H, W, C) với giá trị 0-255\n",
    "        \"\"\"\n",
    "        # Chuyển sang Float ngay từ đầu để tính toán chính xác\n",
    "        frames = frames.float() \n",
    "\n",
    "        if self.is_train:\n",
    "            frames = self._random_resized_crop(frames)\n",
    "            frames = self._color_jitter(frames)\n",
    "            frames = self._random_erasing(frames)\n",
    "            if random.random() < 0.5:\n",
    "                frames = TF.hflip(frames)\n",
    "        else:\n",
    "            # SỬA LỖI Ở ĐÂY: Permute trước khi interpolate\n",
    "            frames = frames.permute(0, 3, 1, 2) # (T, C, H, W)\n",
    "            frames = F.interpolate(\n",
    "                frames,\n",
    "                size=(self.image_size, self.image_size),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            )\n",
    "            frames = frames.permute(0, 2, 3, 1) # Trả về (T, H, W, C) để đồng nhất với normalize\n",
    "\n",
    "        # ---------- NORMALIZE ----------\n",
    "        # Hàm _normalize của bạn nhận (T, H, W, C) và trả về (T, C, H, W) -> Rất chuẩn\n",
    "        frames = self._normalize(frames)\n",
    "        return frames\n",
    "\n",
    "    def _random_resized_crop(self, frames):\n",
    "        \"\"\"Random crop rồi resize về 224x224 - CONSISTENT\"\"\"\n",
    "        T, H, W, C = frames.shape\n",
    "\n",
    "        # Random scale và position (CÙNG cho tất cả frames)\n",
    "        scale = random.uniform(self.crop_scale[0], self.crop_scale[1])\n",
    "        crop_h, crop_w = int(H * scale), int(W * scale)\n",
    "\n",
    "        top = random.randint(0, H - crop_h)\n",
    "        left = random.randint(0, W - crop_w)\n",
    "\n",
    "        # Crop tất cả frames GIỐNG NHAU\n",
    "        frames = frames[:, top:top+crop_h, left:left+crop_w, :]\n",
    "\n",
    "        # Resize về 224x224\n",
    "        # (T, H, W, C) -> (T, C, H, W) for interpolate\n",
    "        frames = frames.permute(0, 3, 1, 2).float()\n",
    "        frames = F.interpolate(frames, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        # (T, C, H, W) -> (T, H, W, C)\n",
    "        frames = frames.permute(0, 2, 3, 1)\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def _color_jitter(self, frames):\n",
    "        \"\"\"Color jitter - CONSISTENT cho tất cả frames\"\"\"\n",
    "        # Random parameters (CÙNG cho tất cả frames)\n",
    "        brightness_factor = 1.0 + random.uniform(-self.brightness, self.brightness)\n",
    "        contrast_factor = 1.0 + random.uniform(-self.contrast, self.contrast)\n",
    "        saturation_factor = 1.0 + random.uniform(-self.saturation, self.saturation)\n",
    "\n",
    "        frames = frames.float()\n",
    "\n",
    "        # Brightness\n",
    "        frames = frames * brightness_factor\n",
    "\n",
    "        # Contrast\n",
    "        mean = frames.mean(dim=(1, 2), keepdim=True)\n",
    "        frames = (frames - mean) * contrast_factor + mean\n",
    "\n",
    "        # Saturation\n",
    "        gray = frames.mean(dim=-1, keepdim=True)\n",
    "        frames = gray + (frames - gray) * saturation_factor\n",
    "\n",
    "        # Clamp to valid range\n",
    "        frames = torch.clamp(frames, 0, 255)\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def _random_erasing(self, frames):\n",
    "        \"\"\"\n",
    "        Random Erasing CONSISTENT across frames\n",
    "        frames: (T, H, W, C), float [0,1]\n",
    "        \"\"\"\n",
    "        if random.random() > self.erase_prob:\n",
    "            return frames\n",
    "    \n",
    "        T, H, W, C = frames.shape\n",
    "        area = H * W\n",
    "    \n",
    "        erase_area = random.uniform(*self.erase_scale) * area\n",
    "        aspect_ratio = random.uniform(*self.erase_ratio)\n",
    "    \n",
    "        h = int(round((erase_area * aspect_ratio) ** 0.5))\n",
    "        w = int(round((erase_area / aspect_ratio) ** 0.5))\n",
    "    \n",
    "        if h <= 0 or w <= 0 or h >= H or w >= W:\n",
    "            return frames\n",
    "    \n",
    "        top = random.randint(0, H - h)\n",
    "        left = random.randint(0, W - w)\n",
    "    \n",
    "        # erase value trong [0,1]\n",
    "        erase_value = torch.randint(0, 256, (1,1,1,C), device=frames.device).float()\n",
    "    \n",
    "        frames[:, top:top+h, left:left+w, :] = erase_value\n",
    "        return frames\n",
    "    def _normalize(self, frames):\n",
    "        \"\"\"\n",
    "        frames: (T, H, W, C) in [0,255]\n",
    "        return: (T, C, H, W) normalized\n",
    "        \"\"\"\n",
    "        frames = frames.float() / 255.0\n",
    "        frames = frames.permute(0, 3, 1, 2)  # (T, C, H, W)\n",
    "    \n",
    "        mean = torch.tensor(self.mean, device=frames.device).view(1, 3, 1, 1)\n",
    "        std = torch.tensor(self.std, device=frames.device).view(1, 3, 1, 1)\n",
    "    \n",
    "        return (frames - mean) / std\n",
    "\n",
    "def denormalize(frames, mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)):\n",
    "    frames = frames.clone()\n",
    "    for c in range(frames.shape[1]):\n",
    "        frames[:, c] = frames[:, c] * std[c] + mean[c]\n",
    "    return frames.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd17b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoOneClipDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        num_frames=16,\n",
    "        image_size=224,\n",
    "        is_train=False\n",
    "    ):\n",
    "        self.root = Path(root)\n",
    "        self.num_frames = num_frames\n",
    "        self.is_train = is_train\n",
    "\n",
    "        # video-level augmentation (bạn đã có VideoAugmentation)\n",
    "        self.transform = VideoAugmentation(\n",
    "            image_size=image_size,\n",
    "            is_train=is_train\n",
    "        )\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "        # load class\n",
    "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "        # collect samples\n",
    "        self.samples = []\n",
    "        self.label_idx = []\n",
    "        for cls in self.classes:\n",
    "            cls_dir = self.root / cls\n",
    "            for video_dir in sorted([d for d in cls_dir.iterdir() if d.is_dir()]):\n",
    "                frame_paths = sorted([\n",
    "                    p for p in video_dir.iterdir()\n",
    "                    if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}\n",
    "                ])\n",
    "                if len(frame_paths) > 0:\n",
    "                    self.samples.append((frame_paths, self.class_to_idx[cls]))\n",
    "                    self.label_idx.append(self.class_to_idx[cls])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def uniform_clip_sampling(self, frame_paths):\n",
    "        total = len(frame_paths)\n",
    "        idxs = []\n",
    "        \n",
    "        if total >= self.num_frames:\n",
    "            # Chia video thành n đoạn bằng nhau\n",
    "            seg_size = total / self.num_frames\n",
    "            for i in range(self.num_frames):\n",
    "                start = int(i * seg_size)\n",
    "                end = int((i + 1) * seg_size)\n",
    "                if self.is_train:\n",
    "                    # Training: Lấy ngẫu nhiên 1 frame trong đoạn\n",
    "                    idxs.append(random.randint(start, end - 1))\n",
    "                else:\n",
    "                    # Validation/Inference: Lấy frame chính giữa đoạn cho ổn định\n",
    "                    idxs.append((start + end) // 2)\n",
    "        else:\n",
    "            # Nếu video ngắn hơn số frame cần thiết: Lấy hết và padding frame cuối\n",
    "            idxs = list(range(total)) + [total - 1] * (self.num_frames - total)\n",
    "            \n",
    "        return [frame_paths[i] for i in idxs]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths, label = self.samples[idx]\n",
    "    \n",
    "        frames = self.uniform_clip_sampling(frame_paths)\n",
    "    \n",
    "        imgs = []\n",
    "        for f in frames:\n",
    "            img = Image.open(f).convert(\"RGB\")\n",
    "            imgs.append(torch.from_numpy(np.array(img)))  # CHỈ to_tensor\n",
    "    \n",
    "        video = torch.stack(imgs)  # (T, C, H, W)\n",
    "        video = self.transform(video)  # VideoAugmentation\n",
    "    \n",
    "        return video, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestOneClipDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset cho VideoMAE multi-clip inference / validation\n",
    "\n",
    "    Output:\n",
    "        clips: (num_clips, T, C, H, W)\n",
    "        label: int\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        num_frames=16,\n",
    "        image_size=224,\n",
    "        is_train=False\n",
    "    ):\n",
    "        self.root = Path(root)\n",
    "        self.num_frames = num_frames\n",
    "        self.is_train = is_train\n",
    "\n",
    "        # video-level augmentation (bạn đã có VideoAugmentation)\n",
    "        self.transform = VideoAugmentation(\n",
    "            image_size=image_size,\n",
    "            is_train=is_train\n",
    "        )\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "        self.video_dirs = sorted([d for d in self.root.iterdir() if d.is_dir()], key=lambda x: int(x.name))\n",
    "        self.video_ids = [int(d.name) for d in self.video_dirs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_dirs)\n",
    "\n",
    "    def uniform_clip_sampling(self, frame_paths):\n",
    "        total = len(frame_paths)\n",
    "        idxs = []\n",
    "        \n",
    "        if total >= self.num_frames:\n",
    "            # Chia video thành n đoạn bằng nhau\n",
    "            seg_size = total / self.num_frames\n",
    "            for i in range(self.num_frames):\n",
    "                start = int(i * seg_size)\n",
    "                end = int((i + 1) * seg_size)\n",
    "                if self.is_train:\n",
    "                    # Training: Lấy ngẫu nhiên 1 frame trong đoạn\n",
    "                    idxs.append(random.randint(start, end - 1))\n",
    "                else:\n",
    "                    # Validation/Inference: Lấy frame chính giữa đoạn cho ổn định\n",
    "                    idxs.append((start + end) // 2)\n",
    "        else:\n",
    "            # Nếu video ngắn hơn số frame cần thiết: Lấy hết và padding frame cuối\n",
    "            idxs = list(range(total)) + [total - 1] * (self.num_frames - total)\n",
    "            \n",
    "        return [frame_paths[i] for i in idxs]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir = self.video_dirs[idx]\n",
    "        video_id = self.video_ids[idx]\n",
    "        frame_paths = sorted([p for p in video_dir.iterdir() if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}])\n",
    "\n",
    "        frames = self.uniform_clip_sampling(frame_paths)\n",
    "    \n",
    "        imgs = []\n",
    "        for f in frames:\n",
    "            img = Image.open(f).convert(\"RGB\")\n",
    "            imgs.append(torch.from_numpy(np.array(img)))  # CHỈ to_tensor\n",
    "    \n",
    "        video = torch.stack(imgs)  # (T, C, H, W)\n",
    "        video = self.transform(video)  # VideoAugmentation\n",
    "\n",
    "        return video, video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7362794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Tuple[torch.Tensor, int]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    videos = torch.stack([item[0] for item in batch])\n",
    "    labels = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
    "    return videos, labels\n",
    "\n",
    "def create_balanced_sampler(dataset):\n",
    "    \"\"\"Create balanced sampler for imbalanced dataset\"\"\"\n",
    "    if hasattr(dataset, 'dataset'):\n",
    "        all_labels = [dataset.dataset.label_idx[i] for i in dataset.indices]\n",
    "    else:\n",
    "        all_labels = dataset.label_idx\n",
    "\n",
    "    class_counts = np.bincount(all_labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = [class_weights[label] for label in all_labels]\n",
    "    sample_weights = torch.FloatTensor(sample_weights)\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    print(f\"Balanced Sampler: class counts min={class_counts.min()}, max={class_counts.max()}\")\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, device, grad_accum_steps=1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    optimizer.zero_grad()\n",
    "    progress = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    for batch_idx, (videos, labels) in enumerate(progress):\n",
    "        videos = videos.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=(device.type == 'cuda')):\n",
    "            outputs = model(videos)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        loss_value = loss.item()\n",
    "        loss = loss / grad_accum_steps\n",
    "        scaler.scale(loss).backward()\n",
    "        should_step = ((batch_idx + 1) % grad_accum_steps == 0) or (batch_idx + 1 == len(loader))\n",
    "        if should_step:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        batch_size = videos.size(0)\n",
    "        total_loss += loss_value * batch_size\n",
    "        progress.set_postfix(loss=f\"{loss_value:.4f}\", acc=f\"{correct / max(total, 1):.4f}\")\n",
    "    avg_loss = total_loss / max(total, 1)\n",
    "    avg_acc = correct / max(total, 1)\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553574e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA_TRAIN = r'/kaggle/input/action-video/data/data_train'\n",
    "PATH_DATA_TEST = r'/kaggle/input/action-video/data/test'\n",
    "NUM_FRAMES = 16\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8 \n",
    "train_dataset = VideoOneClipDataset(PATH_DATA_TRAIN,num_frames=16,image_size=224,is_train=True)\n",
    "balanced_sampler = create_balanced_sampler(train_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,sampler=balanced_sampler, num_workers=2, pin_memory=True)\n",
    "test_dataset = TestOneClipDataset(PATH_DATA_TEST,num_frames=16,image_size=224,is_train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'Train clips: {len(train_dataset)} | Test clips: {len(test_dataset)}')\n",
    "print(f'Train Class count: {len(train_dataset.classes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669687e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "def count_classes(parent_dir):\n",
    "    classes = {}\n",
    "    for folder in os.listdir(parent_dir):\n",
    "        subdir = os.path.join(parent_dir, folder)\n",
    "        num_subdirs = sum(\n",
    "            1 for d in os.listdir(subdir)\n",
    "            if os.path.isdir(os.path.join(subdir, d))\n",
    "        )\n",
    "        classes[folder] = num_subdirs\n",
    "    return classes\n",
    "per_class_counts = count_classes(PATH_DATA_TRAIN)\n",
    "print(\"Samples per class:\", per_class_counts)\n",
    "\n",
    "# 2️⃣ Tạo weight\n",
    "num_classes = len(per_class_counts)\n",
    "total_samples = sum(per_class_counts.values())\n",
    "\n",
    "weights = [total_samples / (num_classes * per_class_counts[c]) for c in per_class_counts]\n",
    "\n",
    "# 3️⃣ Chuyển sang tensor\n",
    "class_weights = torch.tensor(weights, dtype=torch.float).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17802b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "#checkpoint_path = '/kaggle/working/videomae_best.pt'\n",
    "#print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "#checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "#print(checkpoint['acc'])\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "        'MCG-NJU/videomae-base-finetuned-kinetics',\n",
    "        hidden_dropout_prob = 0.1,\n",
    "        attention_probs_dropout_prob =0.1,\n",
    "         num_labels=51,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(DEVICE)\n",
    "#model.load_state_dict(checkpoint['model'])\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights,label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-5,\n",
    "    weight_decay=0.05\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=10\n",
    ")\n",
    "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "best_acc = 0.0\n",
    "best_ckpt = Path('./videomae_best.pt')\n",
    "last_ckpt = Path('./videomae_last.pt')\n",
    "history = {'train_loss': [], 'train_acc': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scaler,\n",
    "        DEVICE,\n",
    "        grad_accum_steps=GRAD_ACCUM_STEPS,\n",
    "    )\n",
    "    scheduler.step()\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    torch.save({'model': model.state_dict(), 'acc': train_acc}, last_ckpt)\n",
    "    if train_acc > best_acc:\n",
    "        best_acc = train_acc\n",
    "        torch.save({'model': model.state_dict(), 'acc': best_acc}, best_ckpt)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}/{EPOCHS} | train_loss={train_loss:.4f} | train_acc={train_acc:.4f}'\n",
    "    )\n",
    "\n",
    "trained_model = model\n",
    "training_history = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f1440",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "classes = train_dataset.classes\n",
    "with torch.no_grad():\n",
    "    for videos, video_ids in tqdm(test_loader, desc=\"Inference\"):\n",
    "        videos = videos.to(DEVICE)\n",
    "        outputs = model(videos)\n",
    "        logits = outputs.logits\n",
    "        preds = logits.argmax(dim=1)\n",
    "        for video_id, pred_idx in zip(video_ids.cpu().numpy(), preds.cpu().numpy()):\n",
    "            pred_class = classes[pred_idx]\n",
    "            predictions.append((video_id, pred_class))\n",
    "\n",
    "predictions.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60fc699",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = Path('./submission.csv')\n",
    "with open(submission_path, 'w') as f:\n",
    "    f.write('id,class\\n')\n",
    "    for video_id, pred_class in predictions:\n",
    "        f.write(f'{video_id},{pred_class}\\n')\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
